{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AVICII-24/Exo-planet_Detect_SEDSCelestia/blob/main/Exoplanet_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "QdOxQsdvkKxn",
        "outputId": "0e246f7f-cd4d-44ea-f9b4-7a11b0467439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of times 1 appears in the 'label' column: 5050\n",
            "Number of times 2 appears in the 'label' column: 37\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nalso remember:\\n1: not an exoplanet\\n2: is an exoplanet\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# import the dataset\n",
        "data_train = pd.read_csv('/content/exoTrain.csv')\n",
        "data_test = pd.read_csv('/content/exoTest.csv')\n",
        "\n",
        "#Checking to see label distribution\n",
        "if 'LABEL' in data_train.columns:\n",
        "    # Count the occurrences of 1 and 2 in the 'label' column\n",
        "    count_1 = (data_train['LABEL'] == 1).sum()\n",
        "    count_2 = (data_train['LABEL'] == 2).sum()\n",
        "\n",
        "    print(f\"Number of times 1 appears in the 'label' column: {count_1}\")\n",
        "    print(f\"Number of times 2 appears in the 'label' column: {count_2}\")\n",
        "'''For Our Reference:'''\n",
        "#Number of times 1 appears in the 'label' column: 5050\n",
        "#Number of times 2 appears in the 'label' column: 37\n",
        "'''\n",
        "also remember:\n",
        "1: not an exoplanet\n",
        "2: is an exoplanet\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell only if you want to generate a 500 record csv file. [seperate from the rest of the code]\n",
        "#Currently im underscaling the usable dataset, only going to be using about 500 values in total, 37 (LABEL=2)*3 and rest(389) as random LABEL=1's\n",
        "import csv\n",
        "\n",
        "# Open the new CSV file for writing\n",
        "with open(\"/content/500size_dataset_exoplanet.csv\", 'w', newline='') as f1:\n",
        "    writer = csv.writer(f1)\n",
        "    writer.writerow(data_train.columns)\n",
        "    num_1s, num_2s = 389, 37\n",
        "    for _, row in data_train.iterrows():\n",
        "        if row['LABEL'] == 1 and num_1s > 0:\n",
        "            writer.writerow(row)\n",
        "            num_1s -= 1\n",
        "        elif row['LABEL'] == 2 and num_2s > 0:\n",
        "            for _ in range(3):  # Oversampling positive indicators by 3 times\n",
        "                writer.writerow(row)\n",
        "            num_2s -= 1\n",
        "\n",
        "        if num_1s == 0 and num_2s == 0:\n",
        "            break\n",
        "\n",
        "print(\"500-size dataset CSV file created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OiCXouyrHzI",
        "outputId": "f57de8ed-3fd6-403b-bcdf-7f3bf65ebb95"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500-size dataset CSV file created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''New data_train_500 specified on the basis of the new file'''\n",
        "data_train_500 = pd.read_csv('/content/500size_dataset_exoplanet.csv')\n",
        "\n",
        "#permute the dataset\n",
        "data_train_500 = np.random.permutation(np.asarray(data_train_500))\n",
        "data_test = np.random.permutation(np.asarray(data_test))\n",
        "\n",
        "\n",
        "#Now we have normalised 500 datapoints for training, and the entire testing dataset(we can even shorten the testing data set to reduce runtime...)\n",
        "#REDUCED THE Y VALUES BACAUSE SIGMOID ONLY CONVERGES TO 1 OR 0 (LABELS WERE 1 AND 2)\n",
        "X_train=data_train_500[:,1:]\n",
        "Y_train=data_train_500[:,0]-1\n",
        "\n",
        "X_test=data_test[:,1:]\n",
        "Y_test=data_test[:,0]-1\n",
        "\n",
        "# Normalize only the feature columns\n",
        "X_train_norm = normalize(X_train, axis=1)\n",
        "X_test_norm = normalize(X_test, axis=1)\n",
        "\n",
        "#Y colums do not need to be normlized\n"
      ],
      "metadata": {
        "id": "sdIBOt0TuAN7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(layers.Conv1D(filters=64, kernel_size=8, activation='relu', input_shape=(3198,1)))\n",
        "model.add(layers.MaxPool1D(pool_size=2, strides=5))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Conv1D(filters=128, kernel_size=6, activation='relu'))\n",
        "model.add(layers.MaxPool1D(pool_size=2, strides=5))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Conv1D(filters=256, kernel_size=4, activation='relu'))\n",
        "model.add(layers.MaxPool1D(pool_size=2, strides=5))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(500, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(250, activation='relu'))\n",
        "model.add(layers.Dense(50, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the Model with Validation Split\n",
        "model.fit(X_train,Y_train,epochs=14,batch_size=16,validation_split=0.2,verbose=1)\n",
        "\n",
        "# Evaluate the Model on Test Data\n",
        "loss, accuracy = model.evaluate(X_test,Y_test, verbose=0)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFtYL2DF_3V-",
        "outputId": "accfece1-0492-4801-f63d-d7529e527841"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 188ms/step - accuracy: 0.6908 - loss: 2.4531 - val_accuracy: 0.7900 - val_loss: 15.7432\n",
            "Epoch 2/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 177ms/step - accuracy: 0.7120 - loss: 2.1747 - val_accuracy: 0.8000 - val_loss: 4.9285\n",
            "Epoch 3/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 262ms/step - accuracy: 0.7788 - loss: 0.6321 - val_accuracy: 0.8500 - val_loss: 4.6186\n",
            "Epoch 4/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 185ms/step - accuracy: 0.8314 - loss: 0.5431 - val_accuracy: 0.7100 - val_loss: 1.2323\n",
            "Epoch 5/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 216ms/step - accuracy: 0.8594 - loss: 0.3465 - val_accuracy: 0.8500 - val_loss: 1.4587\n",
            "Epoch 6/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 246ms/step - accuracy: 0.9038 - loss: 0.2976 - val_accuracy: 0.8700 - val_loss: 0.7977\n",
            "Epoch 7/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 204ms/step - accuracy: 0.9130 - loss: 0.2043 - val_accuracy: 0.8700 - val_loss: 0.5534\n",
            "Epoch 8/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 329ms/step - accuracy: 0.9739 - loss: 0.0666 - val_accuracy: 0.8300 - val_loss: 0.8196\n",
            "Epoch 9/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 184ms/step - accuracy: 0.9661 - loss: 0.1204 - val_accuracy: 0.8500 - val_loss: 0.3270\n",
            "Epoch 10/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 266ms/step - accuracy: 0.9457 - loss: 0.1149 - val_accuracy: 0.8500 - val_loss: 0.6088\n",
            "Epoch 11/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 182ms/step - accuracy: 0.9746 - loss: 0.1237 - val_accuracy: 0.9000 - val_loss: 0.3908\n",
            "Epoch 12/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 261ms/step - accuracy: 0.9482 - loss: 0.1823 - val_accuracy: 0.9000 - val_loss: 0.2344\n",
            "Epoch 13/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 178ms/step - accuracy: 0.9079 - loss: 0.3176 - val_accuracy: 0.8300 - val_loss: 0.6198\n",
            "Epoch 14/14\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 252ms/step - accuracy: 0.9715 - loss: 0.0709 - val_accuracy: 0.9200 - val_loss: 0.1867\n",
            "Test Accuracy: 98.25%\n"
          ]
        }
      ]
    }
  ]
}